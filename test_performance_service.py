#!/usr/bin/env python3
"""
ç®€å•çš„æ€§èƒ½æµ‹è¯•æœåŠ¡éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯é‡æ„åçš„ä»£ç æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import uuid
from datetime import datetime
from typing import Dict, Any

# æ¨¡æ‹Ÿå¯¼å…¥ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦æ­£ç¡®çš„å¯¼å…¥è·¯å¾„ï¼‰
try:
    from app.services.model_management.model_performance_service import (
        ModelPerformanceService, PerformanceTestConfig
    )
    from app.schemas.model_management.performance import (
        TestCreate, SpeedTestRequest, LatencyTestRequest,
        BatchTestRequest, TestComparisonRequest
    )
    print("âœ… æ‰€æœ‰å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    exit(1)


def test_config_constants():
    """æµ‹è¯•é…ç½®å¸¸é‡"""
    print("\nğŸ”§ æµ‹è¯•é…ç½®å¸¸é‡...")
    
    # éªŒè¯é…ç½®å¸¸é‡å­˜åœ¨ä¸”åˆç†
    assert PerformanceTestConfig.DEFAULT_SLEEP_INTERVAL > 0
    assert PerformanceTestConfig.MAX_ROUNDS > 0
    assert PerformanceTestConfig.MAX_CONCURRENT_TESTS > 0
    assert len(PerformanceTestConfig.DEFAULT_PROMPTS) > 0
    
    print(f"   - é»˜è®¤ç¡çœ é—´éš”: {PerformanceTestConfig.DEFAULT_SLEEP_INTERVAL}s")
    print(f"   - æœ€å¤§æµ‹è¯•è½®æ•°: {PerformanceTestConfig.MAX_ROUNDS}")
    print(f"   - æœ€å¤§å¹¶å‘æ•°: {PerformanceTestConfig.MAX_CONCURRENT_TESTS}")
    print(f"   - é»˜è®¤æç¤ºè¯æ•°é‡: {len(PerformanceTestConfig.DEFAULT_PROMPTS)}")
    print("âœ… é…ç½®å¸¸é‡æµ‹è¯•é€šè¿‡")


def test_schema_creation():
    """æµ‹è¯•Schemaåˆ›å»º"""
    print("\nğŸ“‹ æµ‹è¯•Schemaåˆ›å»º...")
    
    # æµ‹è¯•åŸºç¡€æµ‹è¯•åˆ›å»ºSchema
    test_create = TestCreate(
        model_id=uuid.uuid4(),
        test_name="æµ‹è¯•",
        test_type="standard",
        rounds=3,
        test_params={"prompt": "æµ‹è¯•æç¤ºè¯"}
    )
    print(f"   - TestCreate: {test_create.test_name}")
    
    # æµ‹è¯•é€Ÿåº¦æµ‹è¯•è¯·æ±‚Schema
    speed_request = SpeedTestRequest(
        model_id=uuid.uuid4(),
        test_type="standard",
        rounds=3,
        prompt="æµ‹è¯•æç¤ºè¯"
    )
    print(f"   - SpeedTestRequest: {speed_request.test_type}")
    
    # æµ‹è¯•å»¶è¿Ÿæµ‹è¯•è¯·æ±‚Schema
    latency_request = LatencyTestRequest(
        model_id=uuid.uuid4(),
        rounds=5,
        measure_first_token=True
    )
    print(f"   - LatencyTestRequest: rounds={latency_request.rounds}")
    
    # æµ‹è¯•æ‰¹é‡æµ‹è¯•è¯·æ±‚Schema
    batch_request = BatchTestRequest(
        model_ids=[uuid.uuid4(), uuid.uuid4()],
        test_type="standard",
        rounds=3,
        prompt="æ‰¹é‡æµ‹è¯•æç¤ºè¯"
    )
    print(f"   - BatchTestRequest: {len(batch_request.model_ids)} models")
    
    # æµ‹è¯•æ¯”è¾ƒè¯·æ±‚Schema
    comparison_request = TestComparisonRequest(
        test_ids=[uuid.uuid4(), uuid.uuid4()],
        metrics=["avg_response_time", "success_rate"]
    )
    print(f"   - TestComparisonRequest: {len(comparison_request.test_ids)} tests")
    
    print("âœ… Schemaåˆ›å»ºæµ‹è¯•é€šè¿‡")


def test_service_initialization():
    """æµ‹è¯•æœåŠ¡åˆå§‹åŒ–"""
    print("\nğŸ—ï¸ æµ‹è¯•æœåŠ¡åˆå§‹åŒ–...")
    
    # æ¨¡æ‹Ÿæ•°æ®åº“ä¼šè¯ï¼ˆå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„AsyncSessionï¼‰
    class MockDB:
        async def execute(self, query):
            return MockResult()
        
        async def commit(self):
            pass
        
        async def refresh(self, obj):
            pass
        
        def add(self, obj):
            pass
    
    class MockResult:
        def scalar_one_or_none(self):
            return None
        
        def scalars(self):
            return MockScalars()
        
        def scalar_one(self):
            return 0
    
    class MockScalars:
        def all(self):
            return []
    
    # åˆ›å»ºæœåŠ¡å®ä¾‹
    mock_db = MockDB()
    service = ModelPerformanceService(mock_db)
    
    # éªŒè¯åˆå§‹åŒ–
    assert service.db is not None
    assert hasattr(service, '_model_config_cache')
    assert isinstance(service._model_config_cache, dict)
    
    print("   - æ•°æ®åº“ä¼šè¯å·²è®¾ç½®")
    print("   - æ¨¡å‹é…ç½®ç¼“å­˜å·²åˆå§‹åŒ–")
    print("âœ… æœåŠ¡åˆå§‹åŒ–æµ‹è¯•é€šè¿‡")


async def test_cache_methods():
    """æµ‹è¯•ç¼“å­˜æ–¹æ³•"""
    print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜æ–¹æ³•...")
    
    # æ¨¡æ‹Ÿæ•°æ®åº“ä¼šè¯
    class MockDB:
        async def execute(self, query):
            return MockResult()
    
    class MockResult:
        def scalar_one_or_none(self):
            return None
    
    mock_db = MockDB()
    service = ModelPerformanceService(mock_db)
    
    # æµ‹è¯•ç¼“å­˜æ¸…é™¤
    test_id = uuid.uuid4()
    service._model_config_cache[str(test_id)] = "test_config"
    
    # æ¸…é™¤ç‰¹å®šç¼“å­˜
    service._clear_model_config_cache(test_id)
    assert str(test_id) not in service._model_config_cache
    print("   - ç‰¹å®šç¼“å­˜æ¸…é™¤æˆåŠŸ")
    
    # æ·»åŠ å¤šä¸ªç¼“å­˜é¡¹
    service._model_config_cache["test1"] = "config1"
    service._model_config_cache["test2"] = "config2"
    
    # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
    service._clear_model_config_cache()
    assert len(service._model_config_cache) == 0
    print("   - å…¨éƒ¨ç¼“å­˜æ¸…é™¤æˆåŠŸ")
    
    print("âœ… ç¼“å­˜æ–¹æ³•æµ‹è¯•é€šè¿‡")


def test_api_routes():
    """æµ‹è¯•APIè·¯ç”±å®šä¹‰"""
    print("\nğŸŒ æµ‹è¯•APIè·¯ç”±...")
    
    try:
        from app.api.v1.endpoints.model_performance import router
        
        # æ£€æŸ¥è·¯ç”±å™¨å­˜åœ¨
        assert router is not None
        print("   - è·¯ç”±å™¨å·²åˆ›å»º")
        
        # æ£€æŸ¥è·¯ç”±æ•°é‡ï¼ˆåº”è¯¥æœ‰æˆ‘ä»¬å®šä¹‰çš„ç«¯ç‚¹ï¼‰
        routes = router.routes
        print(f"   - è·¯ç”±æ•°é‡: {len(routes)}")
        
        # æ£€æŸ¥ä¸»è¦ç«¯ç‚¹
        route_paths = [route.path for route in routes if hasattr(route, 'path')]
        expected_paths = [
            "/models/{model_id}/performance/tests",
            "/models/{model_id}/performance/latency-tests",
            "/performance/tests/{test_id}",
            "/performance/tests",
            "/performance/tests/batch",
            "/performance/tests/compare"
        ]
        
        for path in expected_paths:
            if any(path in route_path for route_path in route_paths):
                print(f"   - âœ… ç«¯ç‚¹å­˜åœ¨: {path}")
            else:
                print(f"   - âŒ ç«¯ç‚¹ç¼ºå¤±: {path}")
        
        print("âœ… APIè·¯ç”±æµ‹è¯•é€šè¿‡")
        
    except ImportError as e:
        print(f"âŒ APIè·¯ç”±å¯¼å…¥å¤±è´¥: {e}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ModelPerformanceServiceé‡æ„éªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    try:
        # è¿è¡ŒåŒæ­¥æµ‹è¯•
        test_config_constants()
        test_schema_creation()
        test_service_initialization()
        test_api_routes()
        
        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        asyncio.run(test_cache_methods())
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é‡æ„æˆåŠŸå®Œæˆã€‚")
        print("\nğŸ“‹ é‡æ„æ€»ç»“:")
        print("   âœ… é…ç½®ç®¡ç†ç±»å·²åˆ›å»º")
        print("   âœ… é‡å¤ä»£ç å·²æå–")
        print("   âœ… æ–¹æ³•å·²æ‹†åˆ†å’Œä¼˜åŒ–")
        print("   âœ… ç¼“å­˜æœºåˆ¶å·²å®ç°")
        print("   âœ… æ‰¹é‡æµ‹è¯•åŠŸèƒ½å·²æ·»åŠ ")
        print("   âœ… æµ‹è¯•æ¯”è¾ƒåŠŸèƒ½å·²æ·»åŠ ")
        print("   âœ… APIç«¯ç‚¹å·²åˆ›å»º")
        print("   âœ… Schemaå·²æ‰©å±•")
        
        print("\nğŸ”— å¯ç”¨çš„APIç«¯ç‚¹:")
        print("   POST /api/v1/models/{model_id}/performance/tests")
        print("   POST /api/v1/models/{model_id}/performance/latency-tests")
        print("   GET  /api/v1/models/{model_id}/performance/tests")
        print("   GET  /api/v1/performance/tests/{test_id}")
        print("   GET  /api/v1/performance/tests")
        print("   POST /api/v1/performance/tests/batch")
        print("   POST /api/v1/performance/tests/compare")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
